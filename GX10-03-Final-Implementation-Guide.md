
# GX10 ë¡œì»¬ AI ê°œë°œ í™˜ê²½ êµ¬ì¶• ê°€ì´ë“œ

## Revision History
| Date | Version | Description |
|------|---------|-------------|
| 2026-02-01 | v1.0 | ì´ˆì•ˆ ì‘ì„± |
| 2026-02-01 | v2.0 | ë”¥ ë¦¬ì„œì¹˜ ê¸°ë°˜ ì „ë©´ ì¬êµ¬ì„± |

---

## Part A: ìš´ì˜ ê·œì¹™ ë° ì•„í‚¤í…ì²˜

## 0. ë¬¸ì„œ ì„±ê²© ë° ì‚¬ìš© ê·œì¹™

ë³¸ ë¬¸ì„œëŠ” ë‹¤ìŒ ìš©ë„ë¡œ ì‚¬ìš©í•œë‹¤.

- ì‚¬ëŒ ê°œë°œìê°€ ë”°ë¼ì•¼ í•  **ê°œë°œÂ·ìš´ì˜ ì§€ì¹¨**
- AI Agentê°€ ë”°ë¼ì•¼ í•  **í–‰ë™ ê·œì¹™**
- ìë™í™” ì‹œìŠ¤í…œ(n8n/MCP)ì´ í˜¸ì¶œí•  **ì ˆì°¨ ê¸°ì¤€**

â— ë³¸ ë¬¸ì„œëŠ” "ì„¤ëª…ìš©"ì´ ì•„ë‹ˆë¼ **"í–‰ë™ ê¸°ì¤€(Operating Playbook)"**ì´ë‹¤.
â— ì´ ë¬¸ì„œì— ì—†ëŠ” í–‰ë™ì€ **í—ˆìš©ë˜ì§€ ì•ŠëŠ”ë‹¤**.

---

## 1. ìµœìƒìœ„ ëª©í‘œ (ì ˆëŒ€ ê¸°ì¤€)

> **ì¥ê¸° ìœ ì§€ë³´ìˆ˜ê°€ ê°€ëŠ¥í•œ ê³ í’ˆì§ˆ ì½”ë“œ ìƒì‚°**

ë‹¤ìŒ í•­ëª©ì€ ëª¨ë‘ ë¶€ì°¨ ëª©í‘œì´ë‹¤.

- ê°œë°œ ì†ë„
- ìë™í™” ë²”ìœ„
- ë¹„ìš© ì ˆê°
- í¸ì˜ì„±

ì˜ì‚¬ê²°ì • ì‹œ í•­ìƒ **ì½”ë“œ í’ˆì§ˆì´ ìš°ì„ **ì´ë‹¤.

---

## 2. í•˜ë“œì›¨ì–´ ì‚¬ì–‘ ë° ì„±ëŠ¥ íŠ¹ì„±

### 2.1 ASUS Ascent GX10 í™•ì • ì‚¬ì–‘

| í•­ëª© | ì‚¬ì–‘ |
|------|------|
| CPU | ARM v9.2-A (20-core: 10x Cortex-X925 + 10x Cortex-A725) |
| GPU | NVIDIA Blackwell GB10 (1 petaFLOP FP4 sparse) |
| ë©”ëª¨ë¦¬ | 128GB LPDDR5x Unified Memory |
| ë©”ëª¨ë¦¬ ëŒ€ì—­í­ | 273 GB/s (CPU+GPU ê³µìœ ) |
| ìŠ¤í† ë¦¬ì§€ | 1TB NVMe SSD |
| ë„¤íŠ¸ì›Œí¬ | 10GbE + ConnectX-7 (200Gbps QSFP) |
| OS | DGX OS (Ubuntu 24.04 ê¸°ë°˜) |

### 2.2 UMA(Unified Memory Architecture) íŠ¹ì„±

**âš ï¸ ì¤‘ìš”: ì´ ì‚¬ì–‘ì€ ì¼ë°˜ PCì™€ ë‹¤ë¥´ê²Œ ë™ì‘í•œë‹¤.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    128GB Unified Memory Pool                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  CPU ì˜ì—­  â”‚  ê³µìœ  ì˜ì—­  â”‚  GPU ì˜ì—­  â”‚  Buffer Cache  â”‚   â”‚
â”‚  â”‚  (ë™ì )    â”‚   (ë™ì )    â”‚   (ë™ì )   â”‚    (ë™ì )      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    273 GB/s ëŒ€ì—­í­ ê³µìœ                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•µì‹¬ íŠ¹ì„±:**
- CPUì™€ GPUê°€ ë™ì¼í•œ ë©”ëª¨ë¦¬ í’€ì„ ê³µìœ 
- PCIe ì „ì†¡ ë³‘ëª© ì—†ìŒ (NVLink-C2C)
- Buffer Cacheê°€ GPU ë©”ëª¨ë¦¬ë¥¼ ì ìœ í•  ìˆ˜ ìˆìŒ
- ëŒ€ì—­í­(273 GB/s)ì´ ì£¼ìš” ë³‘ëª© â†’ ëŒ€í˜• ëª¨ë¸ì˜ TTFTê°€ ê¹€

### 2.3 ì‹¤ì¸¡ ë²¤ì¹˜ë§ˆí¬ (2025ë…„ ë¦¬ì„œì¹˜ ê¸°ì¤€)

| ëª¨ë¸ | í† í°/ì´ˆ | TTFT | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ë¹„ê³  |
|------|---------|------|--------------|------|
| Qwen2.5-Coder-7B (Q4) | 46 | 22ì´ˆ | ~5GB | ë¹ ë¥¸ ì‘ë‹µìš© |
| Qwen3:32B (Q4) | 9.5 | ~30ì´ˆ | ~20GB | **ê¶Œì¥ ë©”ì¸ ëª¨ë¸** |
| DeepSeek-Coder-V2-16B | 15-20 | ~25ì´ˆ | ~10GB | ëŒ€ì•ˆ |
| Qwen2.5-72B | 4.6 | 133ì´ˆ | ~45GB | ë³µì¡í•œ ì‘ì—…ë§Œ |
| Llama-3.1-70B | 4.6 | ~120ì´ˆ | ~45GB | ëŒ€ì•ˆ |
| DeepSeek-R1-14B (FP8+SGLang) | 83.5 | - | ~10GB | ë°°ì¹˜ ì²˜ë¦¬ì‹œ |

**ê²°ë¡ :**
- 32B ëª¨ë¸ì´ ì‹¤ìš©ì  sweet spot
- 70B+ ëª¨ë¸ì€ TTFTê°€ ê¸¸ì–´ ëŒ€í™”í˜• ì‘ì—…ì— ë¶€ì í•©
- 7B ëª¨ë¸ì€ ìë™ì™„ì„±/ë¹ ë¥¸ ì‘ë‹µì— ì í•©

---

## 3. ê¸°ë³¸ Agent Coding íŒŒì´í”„ë¼ì¸ (ê³ ì •)

ì•„ë˜ íŒŒì´í”„ë¼ì¸ì€ **ëª¨ë“  í”„ë¡œì íŠ¸ì˜ ê¸°ë³¸ ì ˆì°¨**ì´ë©° GX10 ë„ì… ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ **í•­ìƒ ìœ ì§€**í•œë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Agent Coding Pipeline                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ 1ï¸âƒ£ Claude    â”‚â”€â”€â”€â–¶â”‚ 2ï¸âƒ£ GX10      â”‚â”€â”€â”€â–¶â”‚ 3ï¸âƒ£ Claude    â”‚          â”‚
â”‚  â”‚    Code      â”‚    â”‚ Code Brain   â”‚    â”‚    Code      â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚ â€¢ ìš”êµ¬ì‚¬í•­    â”‚    â”‚ â€¢ íŒŒì¼ë³„ êµ¬í˜„ â”‚    â”‚ â€¢ ì „ì²´ ë¦¬ë·°   â”‚          â”‚
â”‚  â”‚ â€¢ ì„¤ê³„       â”‚    â”‚ â€¢ ë°˜ë³µ ìˆ˜ì •   â”‚    â”‚ â€¢ corner caseâ”‚          â”‚
â”‚  â”‚ â€¢ íŒŒì¼ êµ¬ì¡°   â”‚    â”‚ â€¢ í…ŒìŠ¤íŠ¸ í†µê³¼ â”‚    â”‚ â€¢ ë¦¬íŒ©í† ë§   â”‚          â”‚
â”‚  â”‚ â€¢ ì¸í„°í˜ì´ìŠ¤  â”‚    â”‚              â”‚    â”‚              â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â”‚                   â”‚                   â”‚                  â”‚
â”‚         â–¼                   â–¼                   â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                    4ï¸âƒ£ Warp Terminal                     â”‚       â”‚
â”‚  â”‚         ë¹Œë“œ â†’ í…ŒìŠ¤íŠ¸ â†’ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ â†’ Git ì»¤ë°‹          â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### íŒŒì´í”„ë¼ì¸ ëª©ì 
- ì„¤ê³„ ì‚¬ê³ ëŠ” **ê³ ì„±ëŠ¥ ëª¨ë¸** (Claude)
- êµ¬í˜„ ë°˜ë³µì€ **ë¡œì»¬ ì—”ì§„** (GX10)
- í†µí•© íŒë‹¨ì€ ë‹¤ì‹œ **ê³ ì„±ëŠ¥ ëª¨ë¸** (Claude)
- ì‹¤í–‰ì€ **ìë™í™”** (Warp)

---

## 4. ê°œë°œ í™˜ê²½ ë¶„ë¦¬ ì›ì¹™

í™˜ê²½ì€ ë‹¤ìŒ 3ê°œë¡œ **ëª…í™•íˆ ë¶„ë¦¬**í•œë‹¤.

| í™˜ê²½ | ì—­í•  | ìƒì„¸ |
|------|------|------|
| ê°œë°œì PC | ì„¤ê³„ ë° í†µì œ | IDE, Claude Code, Git ê´€ë¦¬ |
| GX10 | ì‹¤í–‰Â·ê²€ì¦ ë‘ë‡Œ | LLM ì¶”ë¡ , í…ŒìŠ¤íŠ¸ ì‹¤í–‰, ì„±ëŠ¥ ê²€ì¦ |
| n8n / MCP | ë¬´ì¸ ìë™í™” | ì›Œí¬í”Œë¡œìš° íŠ¸ë¦¬ê±°, ì•Œë¦¼, ìŠ¤ì¼€ì¤„ë§ |

### GX10ì´ í•˜ì§€ ì•ŠëŠ” ì‘ì—…

- IDE ì œê³µ âŒ
- ìƒì‹œ ëŒ€í™” âŒ
- ìˆ˜ë™ ê°œë°œ âŒ
- ìš”êµ¬ì‚¬í•­ í•´ì„ âŒ
- ì•„í‚¤í…ì²˜ ì„¤ê³„ âŒ

---

## 5. GX10 Brain êµ¬ì„±

### 5.1 Two Brain ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GX10 System                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚    CODE BRAIN       â”‚  OR  â”‚   VISION BRAIN      â”‚          â”‚
â”‚  â”‚    (Native Mode)    â”‚      â”‚   (Docker Mode)     â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚ Ollama (systemd)    â”‚      â”‚ PyTorch Container   â”‚          â”‚
â”‚  â”‚ â€¢ Qwen2.5-Coder-32B â”‚      â”‚ â€¢ Qwen2.5-VL-72B    â”‚          â”‚
â”‚  â”‚ â€¢ DeepSeek-Coder    â”‚      â”‚ â€¢ YOLO/SAM2         â”‚          â”‚
â”‚  â”‚ â€¢ Qwen2.5-Coder-7B  â”‚      â”‚ â€¢ TensorRT          â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚ ë©”ëª¨ë¦¬: 40-50GB     â”‚      â”‚ ë©”ëª¨ë¦¬: 70-90GB     â”‚          â”‚
â”‚  â”‚ í† í°/ì´ˆ: 9-46       â”‚      â”‚ GPU: ìµœëŒ€ í™œìš©      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚  âš ï¸ ë™ì‹œ ì‹¤í–‰ ê¸ˆì§€ - ë‹¨ì¼ Brainë§Œ í™œì„±í™”                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 ì‹¤í–‰ ì •ì±…

1. **ë‹¨ì¼ Brainë§Œ ì‹¤í–‰ ê°€ëŠ¥**
2. **Code + Vision ë™ì‹œ ì‹¤í–‰ ê¸ˆì§€**
3. **Brain ì „í™˜ ì‹œ Buffer Cache í”ŒëŸ¬ì‹œ í•„ìˆ˜**

### 5.3 ì™œ Code Brainì€ Native, Vision Brainì€ Dockerì¸ê°€?

**ë¦¬ì„œì¹˜ ê²°ê³¼:**
- UMA ì•„í‚¤í…ì²˜ì—ì„œ Docker cgroupsê°€ **20-30GB ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œ** ë°œìƒ
- ëŒ€í˜• ëª¨ë¸(>10B)ì€ **native ì‹¤í–‰ì´ 1.6-2.7x íš¨ìœ¨ì **
- Code Brain(32B)ì€ nativeë¡œ ì‹¤í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™”
- Vision Brainì€ ë³µì¡í•œ ì˜ì¡´ì„± ê´€ë¦¬ë¥¼ ìœ„í•´ Docker ì‚¬ìš©

---

## 6. Code Brain ìƒì„¸

### 6.1 ì±…ì„ ì •ì˜

> **Code Brainì€ Execution Planì„ ì…ë ¥ë°›ì•„ ì½”ë“œ êµ¬í˜„ì„ ëê¹Œì§€ ì±…ì„ì§€ëŠ” ë¡œì»¬ ì‹¤í–‰ ì—”ì§„ì´ë‹¤.**

### 6.2 Code Brainì´ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…

- ë””ë ‰í† ë¦¬ ìƒì„±
- íŒŒì¼ ìƒì„± ë° ìˆ˜ì •
- ë‹¤íŒŒì¼ ë™ì‹œ êµ¬í˜„
- í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œ ì¬ìˆ˜ì •
- ë¦¬íŒ©í† ë§
- ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ (128K í† í°)

### 6.3 Code Brainì´ í•˜ì§€ ì•ŠëŠ” ì‘ì—…

- ìš”êµ¬ì‚¬í•­ í•´ì„ âŒ
- ì•„í‚¤í…ì²˜ ì„¤ê³„ âŒ
- ì„ì˜ íŒë‹¨ âŒ

### 6.4 ëª¨ë¸ êµ¬ì„±

| ìš©ë„ | ëª¨ë¸ | í¬ê¸° | í† í°/ì´ˆ |
|------|------|------|---------|
| ë©”ì¸ ì½”ë”© | qwen2.5-coder:32b | ~20GB | ~9.5 |
| ë¹ ë¥¸ ì‘ë‹µ | qwen2.5-coder:7b | ~5GB | ~46 |
| ìˆ˜í•™/ë…¼ë¦¬ | deepseek-coder-v2:16b | ~10GB | ~18 |
| ì„ë² ë”© | nomic-embed-text | ~275MB | - |

### 6.5 ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰

```
Code Brain ì‹¤í–‰ ì‹œ:
â”œâ”€ Ollama ì„œë²„: ~2GB
â”œâ”€ 32B ëª¨ë¸ ë¡œë“œ: ~20GB
â”œâ”€ KV Cache (8K ctx): ~4GB
â”œâ”€ ìš´ì˜ ì˜¤ë²„í—¤ë“œ: ~4GB
â””â”€ ì´ ì˜ˆìƒ: 30-40GB

ë‚¨ì€ ë©”ëª¨ë¦¬: 88-98GB (ì—¬ìœ )
```

---

## 7. Vision Brain ìƒì„¸

### 7.1 ì±…ì„ ì •ì˜

Vision Brainì€ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œë§Œ íŒë‹¨í•œë‹¤.

- ì„±ëŠ¥ ì¬í˜„ì„±
- ìˆ˜ì¹˜ ì•ˆì •ì„±
- íŒŒë¼ë¯¸í„° ì˜í–¥
- í•˜ë“œì›¨ì–´ íš¨ìœ¨

### 7.2 ì£¼ìš” ì‘ì—…

- CUDA / TensorRT ì‹¤í—˜
- latency / throughput ì¸¡ì •
- ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
- ëª¨ë¸ ë¹„êµ ê²€ì¦

### 7.3 ëª¨ë¸ êµ¬ì„±

| ìš©ë„ | ëª¨ë¸ | í¬ê¸° | ë¹„ê³  |
|------|------|------|------|
| Vision LLM | qwen2.5-vl:72b | ~45GB | ê³ í’ˆì§ˆ ë¶„ì„ |
| Vision LLM (ë¹ ë¦„) | qwen2.5-vl:7b | ~5GB | ë¹ ë¥¸ í™•ì¸ |
| Object Detection | YOLOv8x | ~100MB | Ultralytics |
| Segmentation | SAM2-Large | ~2.5GB | Meta |
| Depth | Depth-Anything-V2 | ~1GB | HuggingFace |

### 7.4 ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰

```
Vision Brain ì‹¤í–‰ ì‹œ:
â”œâ”€ Docker ì˜¤ë²„í—¤ë“œ: ~10GB
â”œâ”€ PyTorch + CUDA: ~5GB
â”œâ”€ 72B Vision ëª¨ë¸: ~45GB
â”œâ”€ ì¶”ê°€ ëª¨ë¸ë“¤: ~10GB
â”œâ”€ ë°ì´í„°/ë²„í¼: ~10GB
â””â”€ ì´ ì˜ˆìƒ: 70-90GB

ë‚¨ì€ ë©”ëª¨ë¦¬: 38-58GB
```

---

## 8. Execution Plan ê·œê²©

### 8.1 ì •ì˜

> GX10ì€ **Execution Plan ì—†ì´ ì‘ì—…í•˜ì§€ ì•ŠëŠ”ë‹¤**.

### 8.2 í•„ìˆ˜ í¬í•¨ í•­ëª©

```yaml
# Execution Plan ì˜ˆì‹œ
plan:
  name: "feature-user-auth"
  version: "1.0"
  
structure:
  directories:
    - src/auth/
    - src/auth/handlers/
    - tests/auth/
    
  files:
    - path: src/auth/service.py
      responsibility: "ì¸ì¦ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§"
      dependencies: [src/auth/repository.py]
      
    - path: src/auth/handlers/login.py
      responsibility: "ë¡œê·¸ì¸ í•¸ë“¤ëŸ¬"
      dependencies: [src/auth/service.py]
      
    - path: tests/auth/test_service.py
      responsibility: "ì„œë¹„ìŠ¤ ìœ ë‹› í…ŒìŠ¤íŠ¸"
      test_framework: pytest
      
sequence:
  1: src/auth/repository.py
  2: src/auth/service.py
  3: src/auth/handlers/login.py
  4: tests/auth/test_service.py
  
test_criteria:
  - "pytest tests/auth/ í†µê³¼"
  - "coverage >= 80%"
  
constraints:
  - "ê¸°ì¡´ API ì¸í„°í˜ì´ìŠ¤ ìœ ì§€"
  - "Python 3.11+ í˜¸í™˜"
```

---

## 9. Brain ì „í™˜ ì ˆì°¨

### 9.1 ì „í™˜ íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Brain Switch Procedure                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. í˜„ì¬ Brain ìƒíƒœ ì¡°íšŒ                                     â”‚
â”‚     â””â”€â–¶ GET /api/status                                     â”‚
â”‚                                                             â”‚
â”‚  2. ìš”ì²­ ì‘ì—…ê³¼ Brain ì í•©ì„± ê²€ì‚¬                            â”‚
â”‚     â””â”€â–¶ code ì‘ì—… â†’ Code Brain í•„ìš”                         â”‚
â”‚     â””â”€â–¶ vision ì‘ì—… â†’ Vision Brain í•„ìš”                     â”‚
â”‚                                                             â”‚
â”‚  3. ë¶ˆì¼ì¹˜ ì‹œ:                                               â”‚
â”‚     â”œâ”€â–¶ ê²½ê³  ë¡œê·¸ ê¸°ë¡                                      â”‚
â”‚     â”œâ”€â–¶ í˜„ì¬ Brain ì •ì§€                                     â”‚
â”‚     â”œâ”€â–¶ âš ï¸ Buffer Cache í”ŒëŸ¬ì‹œ (í•„ìˆ˜!)                       â”‚
â”‚     â”œâ”€â–¶ ëª©í‘œ Brain ì‹œì‘                                     â”‚
â”‚     â””â”€â–¶ í—¬ìŠ¤ì²´í¬ í†µê³¼ í›„ ì‹¤í–‰                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 Buffer Cache í”ŒëŸ¬ì‹œ (í•„ìˆ˜)

```bash
# Brain ì „í™˜ ì „ ë°˜ë“œì‹œ ì‹¤í–‰
sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'
sleep 5  # ì•ˆì •í™” ëŒ€ê¸°
```

**âš ï¸ ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ë©´ ìƒˆ Brainì´ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ**

---

## 10. ì›Œí¬í”Œë¡œìš°

### 10.1 ê¸°ë³¸ ê°œë°œ

```
ê°œë°œì PC
  â””â”€â–¶ Claude Code (ì„¤ê³„)
      â””â”€â–¶ PC ë¡œì»¬ LLM ë˜ëŠ” GX10
          â””â”€â–¶ Warp (í…ŒìŠ¤íŠ¸)
              â””â”€â–¶ Git ì»¤ë°‹
```

### 10.2 ëŒ€ê·œëª¨ êµ¬í˜„/ìˆ˜ì •

```
ê°œë°œì PC
  â””â”€â–¶ Execution Plan ì‘ì„±
      â””â”€â–¶ GX10 Code Brain í˜¸ì¶œ
          â””â”€â–¶ êµ¬í˜„ ê²°ê³¼ ìˆ˜ì‹ 
              â””â”€â–¶ Claude Code ë¦¬ë·°
                  â””â”€â–¶ Git ì»¤ë°‹
```

### 10.3 ì˜ìƒì²˜ë¦¬ ê²€ì¦

```
ê°œë°œì PC / n8n íŠ¸ë¦¬ê±°
  â””â”€â–¶ Brain ì „í™˜ (Vision)
      â””â”€â–¶ Vision Brain ì‹¤í–‰
          â””â”€â–¶ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
              â””â”€â–¶ ê²°ê³¼ ì•Œë¦¼
```

---

## 11. ìš°ì„ ìˆœìœ„ ë° ì •ì±…

### 11.1 ìš°ì„ ìˆœìœ„ ê·œì¹™

1. ì™¸ë¶€ ì‘ì—… ì§€ì‹œ **ìµœìš°ì„ **
2. ì§„í–‰ ì¤‘ì¸ ì‘ì—… ì™„ë£Œ
3. Idle Improvement (ì„ íƒì )

### 11.2 ë³€ê²½ ì •ì±…

- Brain ë‹¨ìœ„ ì—…ë°ì´íŠ¸ **ê°€ëŠ¥**
- ëª¨ë¸ êµì²´ **ê°€ëŠ¥**
- ë¡¤ë°± **ê°€ëŠ¥**
- ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ë³€ê²½ **ë¶ˆê°€**

---

## 12. ìµœì¢… ì •ì˜

> **ì´ ì‹œìŠ¤í…œì€ "AIë¥¼ ë§ì´ ì“°ëŠ” êµ¬ì¡°"ê°€ ì•„ë‹ˆë¼**
> **"ì½”ë“œ í’ˆì§ˆì„ ì§€í‚¤ê¸° ìœ„í•´ AIë¥¼ í†µì œí•˜ëŠ” êµ¬ì¡°"ì´ë‹¤.**

---

---

## Part B: ê¸°ìˆ  êµ¬í˜„ ê°€ì´ë“œ

## Phase 1: ì´ˆê¸° ì„¤ì • ë° ì‹œìŠ¤í…œ í™•ì¸

### 1.1 ì²« ë¶€íŒ… ë° ì´ˆê¸° ì„¤ì •

GX10ì€ **DGX OS** (Ubuntu 24.04 ê¸°ë°˜)ê°€ í”„ë¦¬ë¡œë“œë˜ì–´ ì¶œí•˜ëœë‹¤.

```bash
# ì²« ë¶€íŒ… ì‹œ Wi-Fi í•«ìŠ¤íŒŸ ìë™ ìƒì„±
# Quick Start Guideì˜ SSID/Passwordë¡œ ì ‘ì†
# ë¸Œë¼ìš°ì €ì—ì„œ http://spark-xxxx.local ì ‘ì†í•˜ì—¬ ì´ˆê¸° ì„¤ì •:
#   - hostname ì„¤ì • (ì˜ˆ: gx10-brain)
#   - username/password ì„¤ì •
#   - ë„¤íŠ¸ì›Œí¬ ì„¤ì • (ê³ ì • IP ê¶Œì¥)
#   - ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ìë™ ì§„í–‰ í›„ ì¬ë¶€íŒ…
```

### 1.2 ì‹œìŠ¤í…œ í™•ì¸

```bash
# ì‹œìŠ¤í…œ ì •ë³´
uname -a
cat /etc/os-release

# CPU í™•ì¸
lscpu | grep -E "Architecture|Model name|CPU\(s\)"

# GPU í™•ì¸
nvidia-smi

# ë©”ëª¨ë¦¬ í™•ì¸ (119GiB â‰ˆ 128GB)
free -h

# ì•„í‚¤í…ì²˜ í™•ì¸ (aarch64)
uname -m
```

### 1.3 ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ë° í•„ìˆ˜ íŒ¨í‚¤ì§€

```bash
# DGX OS ì—…ë°ì´íŠ¸
sudo apt update && sudo apt upgrade -y

# í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜
sudo apt install -y \
    build-essential \
    cmake \
    git \
    curl \
    wget \
    htop \
    btop \
    tmux \
    vim \
    neovim \
    tree \
    jq \
    unzip \
    net-tools \
    openssh-server \
    python3-pip \
    python3-venv
```

### 1.4 ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

```bash
# ê¸°ë³¸ êµ¬ì¡°
sudo mkdir -p /gx10/{brains,runtime,api,automation,system}

# Code Brain ë””ë ‰í† ë¦¬
sudo mkdir -p /gx10/brains/code/{models,prompts,execution,logs}

# Vision Brain ë””ë ‰í† ë¦¬
sudo mkdir -p /gx10/brains/vision/{models,cuda,benchmarks,logs}

# ëŸ°íƒ€ì„
sudo mkdir -p /gx10/runtime/{locks,logs}

# API
sudo mkdir -p /gx10/api

# ìë™í™”
sudo mkdir -p /gx10/automation/{n8n,mcp}

# ì‹œìŠ¤í…œ
sudo mkdir -p /gx10/system/{monitoring,update,backup}

# ì†Œìœ ê¶Œ ì„¤ì •
sudo chown -R $USER:$USER /gx10
```

### 1.5 í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
cat >> ~/.bashrc << 'EOF'

# === GX10 AI System Configuration ===
export GX10_HOME="/gx10"
export OLLAMA_MODELS="/gx10/brains/code/models"
export HF_HOME="/gx10/brains/vision/models/huggingface"
export TORCH_HOME="/gx10/brains/vision/models/torch"

# CUDA (DGX OSì— ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
export PATH="/usr/local/cuda/bin:$PATH"
export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"

# Aliases
alias gx="cd /gx10"
alias brain-status="/gx10/api/status.sh"
alias brain-switch="/gx10/api/switch.sh"
EOF

source ~/.bashrc
```

### 1.6 Docker ì„¤ì •

```bash
# Docker ìƒíƒœ í™•ì¸ (DGX OSì— í”„ë¦¬ì¸ìŠ¤í†¨)
docker --version

# ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER
newgrp docker

# GPU ì ‘ê·¼ í…ŒìŠ¤íŠ¸
docker run --rm --gpus all nvidia/cuda:12.0-base-ubuntu22.04 nvidia-smi
```

### 1.7 SSH ì„¤ì •

```bash
# SSH ì„œë²„ í™œì„±í™”
sudo systemctl enable ssh
sudo systemctl start ssh

# ë°©í™”ë²½ ì„¤ì •
sudo ufw allow ssh
sudo ufw allow 11434/tcp  # Ollama
sudo ufw allow 8080/tcp   # Open WebUI
sudo ufw allow 5678/tcp   # n8n
sudo ufw enable
```

---

## Phase 2: Code Brain êµ¬ì¶• (Native Mode)

### 2.1 Ollama ì„¤ì¹˜

```bash
# Ollama ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# ë²„ì „ í™•ì¸
ollama --version
```

### 2.2 Ollama ì„œë¹„ìŠ¤ ì„¤ì •

```bash
# systemd ì˜¤ë²„ë¼ì´ë“œ ìƒì„±
sudo mkdir -p /etc/systemd/system/ollama.service.d
sudo tee /etc/systemd/system/ollama.service.d/override.conf << EOF
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_MODELS=/gx10/brains/code/models"
Environment="OLLAMA_KEEP_ALIVE=24h"
Environment="OLLAMA_NUM_PARALLEL=2"
Environment="OLLAMA_MAX_LOADED_MODELS=2"
EOF

sudo systemctl daemon-reload
sudo systemctl restart ollama
sudo systemctl enable ollama

# ìƒíƒœ í™•ì¸
sudo systemctl status ollama
curl http://localhost:11434/api/version
```

### 2.3 ì½”ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# ë©”ì¸ ì½”ë”© ëª¨ë¸ (32B) - ê¶Œì¥
# ì†Œìš” ì‹œê°„: ~30ë¶„, ìš©ëŸ‰: ~20GB
ollama pull qwen2.5-coder:32b

# ë¹ ë¥¸ ì‘ë‹µìš© (7B)
# ì†Œìš” ì‹œê°„: ~10ë¶„, ìš©ëŸ‰: ~5GB
ollama pull qwen2.5-coder:7b

# ëŒ€ì•ˆ: DeepSeek (ìˆ˜í•™/ë…¼ë¦¬ ê°•ì )
ollama pull deepseek-coder-v2:16b

# ì„ë² ë”© ëª¨ë¸ (ì½”ë“œ ê²€ìƒ‰ìš©)
ollama pull nomic-embed-text

# ì„¤ì¹˜ í™•ì¸
ollama list
```

### 2.4 ëª¨ë¸ í…ŒìŠ¤íŠ¸

```bash
# 32B ëª¨ë¸ í…ŒìŠ¤íŠ¸
time ollama run qwen2.5-coder:32b "Write a Python function to calculate fibonacci numbers with memoization" --verbose

# ì˜ˆìƒ ê²°ê³¼:
# - TTFT: 20-40ì´ˆ
# - í† í°/ì´ˆ: 8-12
# - ë©”ëª¨ë¦¬ ì‚¬ìš©: ~25GB
```

### 2.5 Open WebUI ì„¤ì¹˜ (ì„ íƒì‚¬í•­)

```bash
# Open WebUI ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
    --name open-webui \
    --restart unless-stopped \
    -p 8080:8080 \
    -v /gx10/brains/code/webui:/app/backend/data \
    -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
    --add-host=host.docker.internal:host-gateway \
    ghcr.io/open-webui/open-webui:main

# ì ‘ì†: http://gx10-ip:8080
```

---

## Phase 3: Vision Brain êµ¬ì¶• (Docker Mode)

### 3.1 Vision Brain Docker ì´ë¯¸ì§€ ë¹Œë“œ

```bash
# Dockerfile ìƒì„±
cat > /gx10/brains/vision/Dockerfile << 'EOF'
FROM nvcr.io/nvidia/pytorch:24.01-py3

# ì‘ì—… ë””ë ‰í† ë¦¬
WORKDIR /workspace

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€
RUN pip install --no-cache-dir \
    opencv-python \
    opencv-contrib-python \
    ultralytics \
    transformers \
    accelerate \
    timm \
    einops \
    matplotlib \
    seaborn \
    jupyter

# SAM2 ì„¤ì¹˜
RUN pip install --no-cache-dir git+https://github.com/facebookresearch/segment-anything-2.git

# í¬íŠ¸
EXPOSE 8888

# ê¸°ë³¸ ëª…ë ¹
CMD ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]
EOF
```

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
cd /gx10/brains/vision
docker build -t gx10-vision-brain:latest .
```

### 3.2 Vision ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# Qwen2.5-VL ëª¨ë¸ (Ollamaë¡œ)
ollama pull qwen2.5-vl:7b
ollama pull qwen2.5-vl:72b  # ê³ í’ˆì§ˆ ë¶„ì„ìš©

# HuggingFace ëª¨ë¸ (ì§ì ‘ ë‹¤ìš´ë¡œë“œ)
pip install huggingface_hub[cli]

# SAM2 ì²´í¬í¬ì¸íŠ¸
mkdir -p /gx10/brains/vision/models/sam2
wget -P /gx10/brains/vision/models/sam2 \
    https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
```

### 3.3 Vision Brain ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```bash
cat > /gx10/brains/vision/run.sh << 'EOF'
#!/bin/bash
docker run -d \
    --name vision-brain \
    --gpus all \
    --shm-size=16g \
    -p 8888:8888 \
    -v /gx10/brains/vision/models:/workspace/models \
    -v /gx10/brains/vision/benchmarks:/workspace/benchmarks \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    gx10-vision-brain:latest
EOF
chmod +x /gx10/brains/vision/run.sh
```

---

## Phase 4: Brain ì „í™˜ API êµ¬ì¶•

### 4.1 ìƒíƒœ ê´€ë¦¬

```bash
# ì´ˆê¸° ìƒíƒœ íŒŒì¼ ìƒì„±
cat > /gx10/runtime/active_brain.json << 'EOF'
{
  "active_brain": "none",
  "started_at": null,
  "last_task": null
}
EOF
```

### 4.2 ìƒíƒœ ì¡°íšŒ ìŠ¤í¬ë¦½íŠ¸

```bash
cat > /gx10/api/status.sh << 'EOF'
#!/bin/bash

echo "=== GX10 Brain Status ==="
echo ""

# ë©”ëª¨ë¦¬ ìƒíƒœ
echo "ğŸ“Š Memory:"
free -h | grep -E "Mem|Swap"
echo ""

# GPU ìƒíƒœ
echo "ğŸ® GPU:"
nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu --format=csv,noheader
echo ""

# Ollama ìƒíƒœ (Code Brain)
echo "ğŸ§  Code Brain (Ollama):"
if systemctl is-active --quiet ollama; then
    echo "  Status: âœ… Running"
    ollama ps 2>/dev/null || echo "  Models: None loaded"
else
    echo "  Status: âŒ Stopped"
fi
echo ""

# Vision Brain ìƒíƒœ
echo "ğŸ‘ï¸ Vision Brain (Docker):"
if docker ps -q -f name=vision-brain > /dev/null 2>&1; then
    echo "  Status: âœ… Running"
    docker stats vision-brain --no-stream --format "  Memory: {{.MemUsage}}"
else
    echo "  Status: âŒ Stopped"
fi
echo ""

# í˜„ì¬ í™œì„± Brain
echo "ğŸ¯ Active Brain:"
cat /gx10/runtime/active_brain.json | jq -r '.active_brain'
EOF
chmod +x /gx10/api/status.sh
```

### 4.3 Brain ì „í™˜ ìŠ¤í¬ë¦½íŠ¸

```bash
cat > /gx10/api/switch.sh << 'EOF'
#!/bin/bash

TARGET=$1

if [ -z "$TARGET" ]; then
    echo "Usage: switch.sh [code|vision|none]"
    exit 1
fi

echo "=== Switching to $TARGET Brain ==="

# 1. í˜„ì¬ Brain ì •ì§€
echo "[1/4] Stopping current brains..."

# Ollama ëª¨ë¸ ì–¸ë¡œë“œ
ollama ps 2>/dev/null | tail -n +2 | awk '{print $1}' | while read model; do
    ollama stop $model 2>/dev/null
done

# Vision Brain ì»¨í…Œì´ë„ˆ ì •ì§€
docker stop vision-brain 2>/dev/null
docker rm vision-brain 2>/dev/null

# 2. Buffer Cache í”ŒëŸ¬ì‹œ (ì¤‘ìš”!)
echo "[2/4] Flushing buffer cache..."
sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'
sleep 3

# 3. ëª©í‘œ Brain ì‹œì‘
echo "[3/4] Starting $TARGET brain..."

case $TARGET in
    code)
        sudo systemctl start ollama
        sleep 5
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        curl -s http://localhost:11434/api/generate -d '{"model":"qwen2.5-coder:32b","prompt":"hello","stream":false}' > /dev/null
        ;;
    vision)
        /gx10/brains/vision/run.sh
        sleep 10
        ;;
    none)
        sudo systemctl stop ollama
        ;;
esac

# 4. ìƒíƒœ ì—…ë°ì´íŠ¸
echo "[4/4] Updating state..."
cat > /gx10/runtime/active_brain.json << INNER_EOF
{
  "active_brain": "$TARGET",
  "started_at": "$(date -Iseconds)",
  "last_task": null
}
INNER_EOF

echo "=== Switch Complete ==="
/gx10/api/status.sh
EOF
chmod +x /gx10/api/switch.sh
```

---

## Phase 5: n8n ìë™í™” ì—°ë™

### 5.1 n8n ì„¤ì¹˜

```bash
# n8n Docker ì»¨í…Œì´ë„ˆ
docker run -d \
    --name n8n \
    --restart unless-stopped \
    -p 5678:5678 \
    -v /gx10/automation/n8n:/home/node/.n8n \
    -e GENERIC_TIMEZONE="Asia/Seoul" \
    --add-host=host.docker.internal:host-gateway \
    docker.n8n.io/n8nio/n8n

# ì ‘ì†: http://gx10-ip:5678
```

### 5.2 n8n Ollama ì—°ë™ ì„¤ì •

n8n UIì—ì„œ:
1. **Credentials** â†’ **Add Credential** â†’ **Ollama API**
2. Base URL: `http://host.docker.internal:11434`
3. í…ŒìŠ¤íŠ¸ ì—°ê²°

### 5.3 ì˜ˆì œ ì›Œí¬í”Œë¡œìš°: Webhook â†’ Code Brain

```json
{
  "name": "Code Brain Execution",
  "nodes": [
    {
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "parameters": {
        "httpMethod": "POST",
        "path": "execute-code"
      }
    },
    {
      "name": "Ollama Chat",
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "parameters": {
        "model": "qwen2.5-coder:32b",
        "baseUrl": "http://host.docker.internal:11434"
      }
    }
  ]
}
```

---

## Phase 6: MCP ì„œë²„ êµ¬ì¶• (ì„ íƒì‚¬í•­)

### 6.1 MCP ì„œë²„ ì„¤ì¹˜

```bash
# MCP Python SDK ì„¤ì¹˜
pip install mcp

# MCP ì„œë²„ ë””ë ‰í† ë¦¬
mkdir -p /gx10/automation/mcp/servers
```

### 6.2 Code Brain MCP ì„œë²„

```python
# /gx10/automation/mcp/servers/code_brain_server.py
from mcp.server import Server
from mcp.types import Tool, TextContent
import httpx
import json

server = Server("code-brain")

@server.tool("execute_code_task")
async def execute_code_task(
    prompt: str,
    model: str = "qwen2.5-coder:32b"
) -> list[TextContent]:
    """Execute a coding task on the local Code Brain"""
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False
            },
            timeout=300.0
        )
        
    result = response.json()
    return [TextContent(type="text", text=result.get("response", ""))]

@server.tool("list_models")
async def list_models() -> list[TextContent]:
    """List available models on Code Brain"""
    
    async with httpx.AsyncClient() as client:
        response = await client.get("http://localhost:11434/api/tags")
        
    models = response.json().get("models", [])
    model_list = "\n".join([m["name"] for m in models])
    return [TextContent(type="text", text=model_list)]

if __name__ == "__main__":
    import asyncio
    asyncio.run(server.run())
```

---

## Phase 7: ì„œë¹„ìŠ¤ ìë™í™”

### 7.1 ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸

```bash
cat > /gx10/system/start-all.sh << 'EOF'
#!/bin/bash
echo "=== GX10 System Starting ==="

# 1. Ollama (Code Brain ê¸°ë³¸)
echo "[1/3] Starting Ollama..."
sudo systemctl start ollama
sleep 5

# 2. n8n
echo "[2/3] Starting n8n..."
docker start n8n 2>/dev/null || docker run -d \
    --name n8n \
    --restart unless-stopped \
    -p 5678:5678 \
    -v /gx10/automation/n8n:/home/node/.n8n \
    --add-host=host.docker.internal:host-gateway \
    docker.n8n.io/n8nio/n8n

# 3. ìƒíƒœ ì´ˆê¸°í™”
echo "[3/3] Initializing state..."
cat > /gx10/runtime/active_brain.json << INNER_EOF
{
  "active_brain": "code",
  "started_at": "$(date -Iseconds)",
  "last_task": null
}
INNER_EOF

echo "=== System Ready ==="
/gx10/api/status.sh
EOF
chmod +x /gx10/system/start-all.sh
```

### 7.2 ë¶€íŒ… ì‹œ ìë™ ì‹œì‘

```bash
# crontab ë“±ë¡
(crontab -l 2>/dev/null; echo "@reboot sleep 60 && /gx10/system/start-all.sh >> /gx10/runtime/logs/startup.log 2>&1") | crontab -

# ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
mkdir -p /gx10/runtime/logs
```

### 7.3 ìƒíƒœ ëª¨ë‹ˆí„°ë§

```bash
cat > /gx10/system/monitoring/health-check.sh << 'EOF'
#!/bin/bash
# ë§¤ 5ë¶„ë§ˆë‹¤ ì‹¤í–‰ (crontab: */5 * * * *)

TIMESTAMP=$(date -Iseconds)
LOG_FILE="/gx10/runtime/logs/health.log"

# Ollama ì²´í¬
if curl -s http://localhost:11434/api/version > /dev/null; then
    OLLAMA_STATUS="OK"
else
    OLLAMA_STATUS="FAIL"
fi

# ë©”ëª¨ë¦¬ ì²´í¬
MEM_USED=$(free -g | awk '/Mem:/{print $3}')
MEM_TOTAL=$(free -g | awk '/Mem:/{print $2}')

# ë¡œê·¸ ê¸°ë¡
echo "$TIMESTAMP | Ollama: $OLLAMA_STATUS | Memory: ${MEM_USED}/${MEM_TOTAL}GB" >> $LOG_FILE
EOF
chmod +x /gx10/system/monitoring/health-check.sh
```

---

## Phase 8: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 8.1 Code Brain í…ŒìŠ¤íŠ¸

```bash
# Brain ì „í™˜
/gx10/api/switch.sh code

# ê°„ë‹¨í•œ ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5-coder:32b",
  "prompt": "Write a Python class for a binary search tree with insert, search, and delete methods. Include docstrings and type hints.",
  "stream": false
}' | jq -r '.response'

# ì„±ëŠ¥ ì¸¡ì •
time curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5-coder:32b",
  "prompt": "Explain the difference between async and sync programming in Python",
  "stream": false
}' > /dev/null
```

### 8.2 Vision Brain í…ŒìŠ¤íŠ¸

```bash
# Brain ì „í™˜
/gx10/api/switch.sh vision

# Jupyter ì ‘ì† í™•ì¸
curl -s http://localhost:8888 | head -1

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ í…ŒìŠ¤íŠ¸
docker exec vision-brain python3 -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
"
```

### 8.3 í†µí•© í…ŒìŠ¤íŠ¸

```bash
# ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ
/gx10/api/status.sh

# Brain ì „í™˜ í…ŒìŠ¤íŠ¸
/gx10/api/switch.sh code
sleep 10
/gx10/api/status.sh

/gx10/api/switch.sh vision
sleep 15
/gx10/api/status.sh

/gx10/api/switch.sh code
```

---

## Part C: ë¹ ë¥¸ ì°¸ì¡°

## ì„œë¹„ìŠ¤ URL

| ì„œë¹„ìŠ¤ | URL | ìš©ë„ |
|--------|-----|------|
| Ollama API | http://localhost:11434 | LLM ì¶”ë¡  |
| Open WebUI | http://localhost:8080 | ì›¹ ì±„íŒ… |
| Jupyter Lab | http://localhost:8888 | Vision ë…¸íŠ¸ë¶ |
| n8n | http://localhost:5678 | ì›Œí¬í”Œë¡œìš° ìë™í™” |

## ëª…ë ¹ì–´ ìš”ì•½

```bash
# ìƒíƒœ í™•ì¸
/gx10/api/status.sh

# Brain ì „í™˜
/gx10/api/switch.sh code     # Code Brain í™œì„±í™”
/gx10/api/switch.sh vision   # Vision Brain í™œì„±í™”
/gx10/api/switch.sh none     # ëª¨ë‘ ë¹„í™œì„±í™”

# ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘
/gx10/system/start-all.sh

# Ollama ì§ì ‘ ì ‘ê·¼
ollama list                  # ëª¨ë¸ ëª©ë¡
ollama ps                    # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸
ollama run qwen2.5-coder:32b # ëŒ€í™”í˜• ì‹¤í–‰
```

## ëª¨ë¸ ê¶Œì¥ ì‚¬ì–‘

| ìš©ë„ | ëª¨ë¸ | ë©”ëª¨ë¦¬ | ì†ë„ |
|------|------|--------|------|
| ì½”ë”© (ë©”ì¸) | qwen2.5-coder:32b | ~20GB | ~9 tok/s |
| ì½”ë”© (ë¹ ë¦„) | qwen2.5-coder:7b | ~5GB | ~46 tok/s |
| Vision (ë¶„ì„) | qwen2.5-vl:72b | ~45GB | ~4 tok/s |
| Vision (ë¹ ë¦„) | qwen2.5-vl:7b | ~5GB | ~30 tok/s |

## ë¬¸ì œ í•´ê²°

### Ollama ì—°ê²° ì‹¤íŒ¨
```bash
sudo systemctl restart ollama
journalctl -u ollama -f
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# Buffer cache ì •ë¦¬
sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'

# ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸ ë° ì •ë¦¬
ollama ps
ollama stop <model-name>
```

### Brain ì „í™˜ ì‹¤íŒ¨
```bash
# ê°•ì œ ì •ë¦¬
docker stop vision-brain 2>/dev/null
docker rm vision-brain 2>/dev/null
sudo systemctl stop ollama
sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'
sleep 10
/gx10/api/switch.sh code
```

---

## Part D: ê°œë°œì PC ì—°ë™

## Aider ì„¤ì • (CLI í˜ì–´ í”„ë¡œê·¸ë˜ë°)

### ì„¤ì¹˜

```bash
# pipxë¡œ ì„¤ì¹˜ (ê¶Œì¥)
pip install pipx
pipx install aider-chat

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ê°œë°œì PCì˜ .bashrc)
export OLLAMA_API_BASE=http://gx10-brain.local:11434
```

### ì‚¬ìš©ë²•

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ Aider ì‹¤í–‰
cd /path/to/project
aider --model ollama_chat/qwen2.5-coder:32b

# âš ï¸ ì¤‘ìš”: Ollama ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì„¤ì • í•„ìˆ˜ (GX10ì—ì„œ)
# /etc/systemd/system/ollama.service.d/override.confì— ì¶”ê°€:
# Environment="OLLAMA_CONTEXT_LENGTH=32768"
```

**Aider ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:**
- Qwen2.5-Coder-32B: **73.7ì ** (GPT-4o: 71ì , Claude 3.5 Sonnet: 84ì )

---

## Continue.dev ì„¤ì • (VS Code)

```json
// ~/.continue/config.json
{
  "models": [
    {
      "title": "GX10 Code Brain",
      "provider": "ollama",
      "model": "qwen2.5-coder:32b",
      "apiBase": "http://gx10-brain.local:11434"
    }
  ],
  "tabAutocompleteModel": {
    "title": "Fast Complete",
    "provider": "ollama",
    "model": "qwen2.5-coder:7b",
    "apiBase": "http://gx10-brain.local:11434"
  },
  "embeddingsProvider": {
    "provider": "ollama",
    "model": "nomic-embed-text",
    "apiBase": "http://gx10-brain.local:11434"
  }
}
```

---

## OpenHands ì„¤ì • (ììœ¨ ì—ì´ì „íŠ¸)

```bash
# GX10ì—ì„œ OpenHands ì‹¤í–‰
docker run -d \
    --name openhands \
    -p 3001:3000 \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ~/.openhands-state:/.openhands-state \
    -e LLM_OLLAMA_BASE_URL="http://host.docker.internal:11434" \
    --add-host=host.docker.internal:host-gateway \
    docker.all-hands.dev/all-hands-ai/openhands:latest
```

**UI ì„¤ì • (http://gx10-ip:3001):**
- Model: `ollama/qwen2.5-coder:32b`
- Base URL: `http://host.docker.internal:11434`
- API Key: `dummy`

âš ï¸ **í•„ìˆ˜:** Ollama ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì†Œ 22,000 í† í°

---

## SSH í„°ë„ (ê°œë°œì PCì—ì„œ)

```bash
# Ollama API í¬ì›Œë”©
ssh -N -L 11434:localhost:11434 user@gx10-brain.local &

# n8n í¬ì›Œë”©
ssh -N -L 5678:localhost:5678 user@gx10-brain.local &
```

---

## Part E: ìµœì¢… ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              ê°œë°œì PC                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Claude Code â”‚  â”‚   VS Code   â”‚  â”‚    Warp     â”‚  â”‚   Gitea     â”‚        â”‚
â”‚  â”‚  (ì„¤ê³„)     â”‚  â”‚ +Continue   â”‚  â”‚ (í„°ë¯¸ë„)    â”‚  â”‚ (Git ì„œë²„)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                   â”‚ SSH / Tailscale                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              GX10 Brain Server                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         128GB Unified Memory                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚  â”‚      CODE BRAIN           â”‚ORâ”‚      VISION BRAIN         â”‚       â”‚   â”‚
â”‚  â”‚  â”‚      (Native)             â”‚  â”‚      (Docker)             â”‚       â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ qwen2.5-coder:32b      â”‚  â”‚  â€¢ qwen2.5-vl:72b         â”‚       â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ qwen2.5-coder:7b       â”‚  â”‚  â€¢ YOLO/SAM2              â”‚       â”‚   â”‚
â”‚  â”‚  â”‚  ë©”ëª¨ë¦¬: 30-40GB          â”‚  â”‚  ë©”ëª¨ë¦¬: 70-90GB          â”‚       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â”‚                     âš ï¸ ë™ì‹œ ì‹¤í–‰ ê¸ˆì§€                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  n8n (ì›Œí¬í”Œë¡œìš°)  â”‚  MCP Server  â”‚  Brain Switch API                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Appendix

## A. ê²€ì¦ëœ ë²¤ì¹˜ë§ˆí¬ ì¶œì²˜

| ì¶œì²˜ | ë‚ ì§œ | ì£¼ìš” ë°ì´í„° |
|------|------|-------------|
| [LMSYS Org](https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/) | 2025-10 | SGLang ë°°ì¹˜, DeepSeek-R1 83.5 tok/s |
| [ProX PC](https://www.proxpc.com/blogs/nvidia-dgx-spark-gb10-performance-test-vs-5090-llm-image-and-video-generation) | 2025-12 | Qwen2.5-72B: 4.6 tok/s, TTFT 133ì´ˆ |
| [NVIDIA Blog](https://developer.nvidia.com/blog/how-nvidia-dgx-sparks-performance-enables-intensive-ai-tasks/) | 2025-10 | Qwen3-235B ë“€ì–¼: 23,477 tok/s |
| [Brandon RC](https://brandonrc.github.io/benchmark-spark/phase1/index.html) | 2025-11 | Docker 20-30GB ì˜¤ë²„í—¤ë“œ |

## B. ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½

| ëª¨ë¸ | ë©”ëª¨ë¦¬ | ì†ë„ | ìš©ë„ |
|------|--------|------|------|
| qwen2.5-coder:32b | ~20GB | ~9.5 tok/s | ë©”ì¸ ì½”ë”© |
| qwen2.5-coder:7b | ~5GB | ~46 tok/s | ìë™ì™„ì„± |
| qwen2.5-vl:72b | ~45GB | ~4.6 tok/s | ì •ë°€ ë¹„ì „ |
| qwen2.5-vl:7b | ~5GB | ~30 tok/s | ë¹ ë¥¸ ë¹„ì „ |

## C. ë¼ì´ì„ ìŠ¤

| êµ¬ì„±ìš”ì†Œ | ë¼ì´ì„ ìŠ¤ | ìƒì—…ì  ì‚¬ìš© |
|----------|----------|-------------|
| Qwen2.5-Coder | Apache 2.0 | âœ… |
| Qwen2.5-VL | Apache 2.0 | âœ… |
| DeepSeek-Coder-V2 | DeepSeek License | âœ… |
| Ollama | MIT | âœ… |
| n8n | Sustainable Use | âš ï¸ ì¡°ê±´ë¶€ |
| OpenHands | MIT | âœ… |

## D. ë²„ì „ ì •ë³´

| êµ¬ì„±ìš”ì†Œ | ê¶Œì¥ ë²„ì „ |
|----------|----------|
| DGX OS | 1.1+ |
| CUDA | 12.x / 13.x |
| Ollama | 0.5+ |
| Docker | 24.x+ |
| Python | 3.11+ |

## E. ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì„¤ì¹˜ ì™„ë£Œ í™•ì¸
- [ ] DGX OS ì´ˆê¸° ì„¤ì •
- [ ] `nvidia-smi` ì •ìƒ ì¶œë ¥
- [ ] Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- [ ] Docker GPU íŒ¨ìŠ¤ìŠ¤ë£¨ í…ŒìŠ¤íŠ¸
- [ ] Brain ì „í™˜ ìŠ¤í¬ë¦½íŠ¸ ë™ì‘
- [ ] n8n + Ollama ì—°ë™
- [ ] ê°œë°œì PC SSH ì ‘ì†

---

**ë¬¸ì„œ ë**

*ë³¸ ë¬¸ì„œëŠ” 2025ë…„ ì‹¤ì¸¡ ë²¤ì¹˜ë§ˆí¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
*í•˜ë“œì›¨ì–´/ì†Œí”„íŠ¸ì›¨ì–´ ì—…ë°ì´íŠ¸ì— ë”°ë¼ ì„±ëŠ¥ì´ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.*

---

## ğŸ“œ ìˆ˜ì • ì´ë ¥

| ì¼ì | ë²„ì „ | ì„¤ëª… | ë¦¬ë·°ì–´ |
|------|------|------|--------|
| 2026-02-01 | 2.0 | ë”¥ ë¦¬ì„œì¹˜ ê¸°ë°˜ ì „ë©´ ì¬êµ¬ì„± | drake |

---

## ğŸ“ ë¬¸ì„œ ì •ë³´

**ì‘ì„±ì**:

- AI: Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)
- í™˜ê²½: MoAI-ADK v11.0.0 (Claude Code + Korean Language Support)
- ì‘ì„±ì¼: 2026-02-01

**ë¦¬ë·°ì–´**:

- drake

**ìˆ˜ì •ì**:
- ìˆ˜ì •ì¼: 2026-02-01
- ìˆ˜ì • ë‚´ìš©: ë¬¸ì„œ í˜•ì‹ í‘œì¤€í™” ë° ì‘ì„±ì ì •ë³´ ë³´ì™„ (omc-planner)

<!-- alfrad review:
  âœ… ì‘ì„±ì ì •ë³´ í‘œì¤€í™” ìœ ì§€
  âœ… v2.0 ë¬¸ì„œë¡œì„œ ë²„ì „ ì •ë³´ ì •í™•íˆ ë°˜ì˜
  âœ… ë”¥ ë¦¬ì„œì¹˜ ê¸°ë°˜ ì¬êµ¬ì„± ì‚¬í•­ ëª…í™•í•¨
  âœ… ìˆ˜ì •ì ì„¹ì…˜ ì¶”ê°€ë¡œ ë³€ê²½ ì¶”ì  ê°œì„ 
  ğŸ’¡ ì°¸ê³ : ìˆ˜ì • ì´ë ¥ í…Œì´ë¸”ì˜ v2.0 ë³€ê²½ ì‚¬í•­ê³¼ ì‘ì„±ì ì •ë³´ ì—°ê³„ í™•ì¸ í•„ìš”
-->
