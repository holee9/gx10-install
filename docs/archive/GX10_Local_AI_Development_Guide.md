# ASUS Ascent GX10 ë¡œì»¬ AI ê°œë°œ í™˜ê²½ êµ¬ì¶• ê°€ì´ë“œ

> **ëª©í‘œ**: êµ¬ë… API ì—†ì´ ì™„ì „ ë¡œì»¬ì—ì„œ ë™ì‘í•˜ëŠ” í†µí•© AI ê°œë°œ ì‹œìŠ¤í…œ  
> **ìš©ë„**: Coding Agent + Vision AIë¥¼ 1ëŒ€ì˜ GX10ì—ì„œ ìš´ìš©

---

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ASUS Ascent GX10                                  â”‚
â”‚         GB10 Grace Blackwell Superchip / 128GB Unified Memory            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        LLM Inference Layer                         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   Ollama    â”‚  â”‚    vLLM     â”‚  â”‚   Docker Model Runner       â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  (Primary)  â”‚  â”‚ (Optional)  â”‚  â”‚   (Alternative)             â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      Coding Agent          â”‚  â”‚        Vision AI                â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Models:                    â”‚  â”‚ Models:                         â”‚   â”‚
â”‚  â”‚ â€¢ Qwen2.5-Coder-32B        â”‚  â”‚ â€¢ Qwen2.5-VL-7B/72B             â”‚   â”‚
â”‚  â”‚ â€¢ DeepSeek-Coder-V2        â”‚  â”‚ â€¢ LLaVA-NeXT                    â”‚   â”‚
â”‚  â”‚ â€¢ Codestral-22B            â”‚  â”‚ â€¢ MiniCPM-V                     â”‚   â”‚
â”‚  â”‚ â€¢ GPT-OSS (option)         â”‚  â”‚                                 â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Agents:                    â”‚  â”‚ Frameworks:                     â”‚   â”‚
â”‚  â”‚ â€¢ Cline (VS Code)          â”‚  â”‚ â€¢ PyTorch + TorchVision         â”‚   â”‚
â”‚  â”‚ â€¢ Aider (CLI)              â”‚  â”‚ â€¢ OpenCV + CUDA                 â”‚   â”‚
â”‚  â”‚ â€¢ Continue.dev             â”‚  â”‚ â€¢ Ultralytics (YOLO)            â”‚   â”‚
â”‚  â”‚ â€¢ OpenHands                â”‚  â”‚ â€¢ SAM2, Depth-Anything          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        Interface Layer                             â”‚  â”‚
â”‚  â”‚   Open WebUI â”‚ Jupyter Lab â”‚ VS Code Server â”‚ ComfyUI (Optional)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 1: ì´ˆê¸° ì„¤ì • ë° ì‹œìŠ¤í…œ í™•ì¸

### 1.1 ì²« ë¶€íŒ… ë° ì´ˆê¸° ì„¤ì •

GX10ì€ **DGX OS 7.2.3** (Ubuntu 24.04 LTS ê¸°ë°˜)ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```bash
# DGX OS ì²« ë¶€íŒ… ì‹œ Wi-Fi í•«ìŠ¤íŒŸ ìë™ ìƒì„±
# Quick Start Guideì˜ SSID/Passwordë¡œ ì ‘ì†
# ë¸Œë¼ìš°ì €ì—ì„œ http://spark-xxxx.local ì ‘ì†í•˜ì—¬ ì´ˆê¸° ì„¤ì • ì§„í–‰
# - hostname, username, password, network ì„¤ì •
# - ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ìë™ ì§„í–‰ í›„ ì¬ë¶€íŒ…

# ì„¤ì • ì™„ë£Œ í›„ SSHë¡œ ì ‘ì†
uname -a  # Linux 6.8.x-dgx kernel
cat /etc/os-release  # DGX OS / Ubuntu 24.04 LTS
```

### 1.2 ì‹œìŠ¤í…œ ì‚¬ì–‘ í™•ì¸

```bash
# ì‹œìŠ¤í…œ ì •ë³´
uname -a
cat /etc/os-release

# CPU í™•ì¸ (ARM v9.2-A: 10x Cortex-X925 + 10x Cortex-A725)
lscpu

# GPU í™•ì¸ (NVIDIA Blackwell GB10)
nvidia-smi

# í†µí•© ë©”ëª¨ë¦¬ í™•ì¸ (128GB)
free -h

# ìŠ¤í† ë¦¬ì§€ í™•ì¸
df -h
lsblk

# ì•„í‚¤í…ì²˜ í™•ì¸ (aarch64)
uname -m
```

### 1.3 ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸

```bash
# DGX OS ì—…ë°ì´íŠ¸ (DGX Dashboard ë˜ëŠ” CLI)
sudo apt update && sudo apt upgrade -y

# í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜ (ëŒ€ë¶€ë¶„ DGX OSì— ì‚¬ì „ ì„¤ì¹˜)
sudo apt install -y \
    build-essential \
    cmake \
    git \
    curl \
    wget \
    htop \
    btop \
    tmux \
    vim \
    neovim \
    tree \
    jq \
    unzip \
    p7zip-full \
    net-tools \
    openssh-server

# Python 3.12 í™•ì¸ (DGX OS ê¸°ë³¸ ë²„ì „)
python3 --version  # Python 3.12.x

# ì¤‘ìš”: DGX OSë„ PEP 668 ì ìš©
# pip install --user ì‚¬ìš© ì‹œ ê²½ê³  ë°œìƒ
# python3-venvë¡œ ê°€ìƒí™˜ê²½ ì‚¬ìš© ê¶Œì¥

# DGX OS ì‚¬ì „ ì„¤ì¹˜ ì»´í¬ë„ŒíŠ¸ í™•ì¸
nvidia-smi              # NVIDIA ë“œë¼ì´ë²„ (ì‚¬ì „ ì„¤ì¹˜)
nvcc --version          # CUDA Toolkit (ì‚¬ì „ ì„¤ì¹˜)
docker --version        # Docker (ì‚¬ì „ ì„¤ì¹˜)
nvidia-ctk --version    # NVIDIA Container Toolkit (ì‚¬ì „ ì„¤ì¹˜)
```

### 1.4 ì‘ì—… ë””ë ‰í† ë¦¬ êµ¬ì„±

```bash
# AI ì›Œí¬ìŠ¤í˜ì´ìŠ¤ êµ¬ì¡°
mkdir -p ~/workspace/{agents,vision,models,data,projects,scripts}
mkdir -p ~/workspace/models/{ollama,huggingface,checkpoints}
mkdir -p ~/workspace/data/{datasets,outputs,cache}

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
cat >> ~/.bashrc << 'EOF'

# === AI Workspace Configuration ===
export WORKSPACE="$HOME/workspace"
export HF_HOME="$WORKSPACE/models/huggingface"
export HF_HUB_CACHE="$WORKSPACE/models/huggingface"
export OLLAMA_MODELS="$WORKSPACE/models/ollama"
export TORCH_HOME="$WORKSPACE/models/checkpoints"

# CUDA paths (DGX OSì— ì‚¬ì „ ì„¤ì¹˜ë˜ì–´ ìˆìŒ)
export PATH="/usr/local/cuda/bin:$PATH"
export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"

# DGX OSì—ì„œëŠ” CUDA_HOME ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ
# í™•ì¸: echo $CUDA_HOME
```

# Convenience aliases
alias ws="cd $WORKSPACE"
alias models="cd $WORKSPACE/models"
alias projects="cd $WORKSPACE/projects"
EOF

source ~/.bashrc
```

### 1.5 Docker ë° NVIDIA Container Toolkit

```bash
# Docker ìƒíƒœ í™•ì¸ (DGX OSì— ì‚¬ì „ ì„¤ì¹˜ë¨)
docker --version
docker info

# NVIDIA Container Toolkit í™•ì¸ (ì‚¬ì „ ì„¤ì¹˜ë¨)
nvidia-ctk --version

# Docker ì„œë¹„ìŠ¤ í™œì„±í™” (ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
sudo systemctl enable docker
sudo systemctl start docker

# ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER
newgrp docker

# GPU ì ‘ê·¼ í…ŒìŠ¤íŠ¸ (DGX OSì—ì„œëŠ” ë°”ë¡œ ì‘ë™)
docker run --rm --gpus all nvidia/cuda:12.0-base-ubuntu24.04 nvidia-smi
```

### 1.6 ì›ê²© ì ‘ì† ì„¤ì •

```bash
# SSH ì„œë²„ í™œì„±í™”
sudo systemctl enable ssh
sudo systemctl start ssh

# (ì„ íƒ) Tailscale VPN ì„¤ì¹˜ - ì™¸ë¶€ ì ‘ì†ìš©
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up
```

---

## Phase 2: LLM ì¶”ë¡  ì¸í”„ë¼ êµ¬ì¶•

### 2.1 Ollama ì„¤ì¹˜ ë° ì„¤ì •

OllamaëŠ” GX10/DGX Sparkì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```bash
# Ollama ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
sudo systemctl status ollama

# ì™¸ë¶€ ì ‘ì† ë° ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
sudo mkdir -p /etc/systemd/system/ollama.service.d
sudo tee /etc/systemd/system/ollama.service.d/override.conf << EOF
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_MODELS=$HOME/workspace/models/ollama"
Environment="OLLAMA_KEEP_ALIVE=24h"
EOF

sudo systemctl daemon-reload
sudo systemctl restart ollama

# ì„¤ì¹˜ í™•ì¸
ollama --version
curl http://localhost:11434/api/version
```

### 2.2 ì½”ë”©ìš© LLM ëª¨ë¸ ì„¤ì¹˜

128GB í†µí•© ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ ëŒ€í˜• ì½”ë”© ëª¨ë¸ êµ¬ì„±:

```bash
# === í•„ìˆ˜ ì½”ë”© ëª¨ë¸ ===

# 1. Qwen2.5-Coder-32B (ìµœê³  ì„±ëŠ¥ ì˜¤í”ˆì†ŒìŠ¤ ì½”ë”© ëª¨ë¸)
#    - 92ê°œ ì´ìƒ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì§€ì›
#    - 128K ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
ollama pull qwen2.5-coder:32b

# 2. DeepSeek-Coder-V2 (ë‹¤êµ­ì–´ ì½”ë”© + ìˆ˜í•™ ê°•ì )
ollama pull deepseek-coder-v2:16b

# 3. Codestral-22B (Mistralì˜ ì½”ë”© íŠ¹í™” ëª¨ë¸)
ollama pull codestral:22b

# === ë¹ ë¥¸ ì‘ë‹µìš© ê²½ëŸ‰ ëª¨ë¸ ===

# 4. Qwen2.5-Coder-7B (ìë™ì™„ì„±, ë¹ ë¥¸ ì‘ë‹µ)
ollama pull qwen2.5-coder:7b

# 5. DeepSeek-Coder-V2-Lite (ê²½ëŸ‰ ëŒ€ì•ˆ)
ollama pull deepseek-coder-v2:lite

# === ë²”ìš© ì¶”ë¡  ëª¨ë¸ (ì„ íƒ) ===

# 6. Qwen2.5-72B (ë²”ìš© ì¶”ë¡ , 128GB ë©”ëª¨ë¦¬ í•„ìš”)
ollama pull qwen2.5:72b

# 7. Llama-3.1-70B (Metaì˜ ëŒ€í˜• ëª¨ë¸)
ollama pull llama3.1:70b

# ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
ollama list

# ëª¨ë¸ í…ŒìŠ¤íŠ¸
ollama run qwen2.5-coder:32b "Write a Python function to calculate fibonacci"
```

### 2.3 Open WebUI ì„¤ì¹˜ (ì›¹ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤)

```bash
# Open WebUI ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
    --name open-webui \
    --restart always \
    --gpus all \
    -p 8080:8080 \
    -v open-webui-data:/app/backend/data \
    -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
    --add-host=host.docker.internal:host-gateway \
    ghcr.io/open-webui/open-webui:main

# ìƒíƒœ í™•ì¸
docker logs -f open-webui

# ì ‘ì†: http://localhost:8080
# ì²« ì ‘ì†ì‹œ ê´€ë¦¬ì ê³„ì • ìƒì„±
```

---

## Phase 3: Coding Agent ì‹œìŠ¤í…œ êµ¬ì¶•

### 3.1 Python ê¸°ë³¸ í™˜ê²½

```bash
# Python ê°œë°œ ë„êµ¬ (Python 3.12 í¬í•¨, DGX OSì— ì‚¬ì „ ì„¤ì¹˜)
sudo apt install -y python3-pip python3-venv python3-dev

# Python ë²„ì „ í™•ì¸
python3 --version  # Python 3.12.x

# pipx ì„¤ì¹˜ (CLI ë„êµ¬ ê´€ë¦¬)
# DGX OSë„ PEP 668 ì ìš©ìœ¼ë¡œ ê°€ìƒí™˜ê²½ í†µí•œ ì„¤ì¹˜ ê¶Œì¥
python3 -m venv ~/.local/pipx-env
source ~/.local/pipx-env/bin/activate
pip install pipx
pipx ensurepath
deactivate
source ~/.bashrc
```

### 3.2 Aider ì„¤ì¹˜ (CLI ì½”ë”© ì—ì´ì „íŠ¸)

AiderëŠ” Git í†µí•© AI í˜ì–´ í”„ë¡œê·¸ë˜ë° ë„êµ¬ì…ë‹ˆë‹¤.

```bash
# Aider ì„¤ì¹˜
pipx install aider-chat

# ë˜ëŠ” ê°€ìƒí™˜ê²½ìœ¼ë¡œ ì„¤ì¹˜
cd ~/workspace/agents
python3 -m venv aider-env
source aider-env/bin/activate
pip install aider-chat

# Aider ì„¤ì • íŒŒì¼
cat > ~/.aider.conf.yml << 'EOF'
# Ollama ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
model: ollama/qwen2.5-coder:32b

# ë¹ ë¥¸ ëª¨ë¸ (ìë™ì™„ì„±ìš©)
weak-model: ollama/qwen2.5-coder:7b

# Git ì„¤ì •
auto-commits: true
dirty-commits: true

# UI ì„¤ì •
dark-mode: true
pretty: true
stream: true
EOF

# Aider ì‹¤í–‰ í…ŒìŠ¤íŠ¸
cd ~/workspace/projects
mkdir test-project && cd test-project
git init
aider --model ollama/qwen2.5-coder:32b
```

### 3.3 Cline ì„¤ì¹˜ (VS Code í™•ì¥)

Clineì€ VS Code ë‚´ ììœ¨ ì½”ë”© ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

```bash
# VS Code ì„¤ì¹˜ (ARM64)
wget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > packages.microsoft.gpg
sudo install -D -o root -g root -m 644 packages.microsoft.gpg /etc/apt/keyrings/packages.microsoft.gpg
sudo sh -c 'echo "deb [arch=arm64 signed-by=/etc/apt/keyrings/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main" > /etc/apt/sources.list.d/vscode.list'
sudo apt update
sudo apt install -y code

# Cline í™•ì¥ ì„¤ì¹˜
code --install-extension saoudrizwan.claude-dev
```

**Cline ì„¤ì •** (VS Code ë‚´ì—ì„œ):
1. Cline ì‚¬ì´ë“œë°” ì—´ê¸°
2. Settings â†’ API Provider â†’ Ollama ì„ íƒ
3. Model: `qwen2.5-coder:32b`
4. Base URL: `http://localhost:11434`

### 3.4 Continue.dev ì„¤ì¹˜ (IDE í†µí•©)

```bash
# Continue í™•ì¥ ì„¤ì¹˜
code --install-extension Continue.continue

# ì„¤ì • íŒŒì¼ ìƒì„±
mkdir -p ~/.continue
cat > ~/.continue/config.json << 'EOF'
{
  "models": [
    {
      "title": "Qwen2.5-Coder-32B (Main)",
      "provider": "ollama",
      "model": "qwen2.5-coder:32b",
      "apiBase": "http://localhost:11434"
    },
    {
      "title": "DeepSeek-Coder-V2",
      "provider": "ollama",
      "model": "deepseek-coder-v2:16b",
      "apiBase": "http://localhost:11434"
    },
    {
      "title": "Codestral",
      "provider": "ollama",
      "model": "codestral:22b",
      "apiBase": "http://localhost:11434"
    }
  ],
  "tabAutocompleteModel": {
    "title": "Fast Autocomplete",
    "provider": "ollama",
    "model": "qwen2.5-coder:7b",
    "apiBase": "http://localhost:11434"
  },
  "embeddingsProvider": {
    "provider": "ollama",
    "model": "nomic-embed-text",
    "apiBase": "http://localhost:11434"
  },
  "reranker": {
    "name": "none"
  }
}
EOF

# ì„ë² ë”© ëª¨ë¸ ì„¤ì¹˜ (ì½”ë“œë² ì´ìŠ¤ ê²€ìƒ‰ìš©)
ollama pull nomic-embed-text
```

### 3.5 OpenHands ì„¤ì¹˜ (ììœ¨ ê°œë°œ ì—ì´ì „íŠ¸)

```bash
# Dockerë¡œ OpenHands ì‹¤í–‰
docker pull ghcr.io/all-hands-ai/openhands:main

# OpenHands ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
cat > ~/workspace/scripts/start-openhands.sh << 'EOF'
#!/bin/bash
docker run -it --rm \
    --name openhands \
    -p 3001:3000 \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ~/workspace/projects:/opt/workspace \
    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=ghcr.io/all-hands-ai/runtime:main \
    -e LLM_MODEL="ollama/qwen2.5-coder:32b" \
    -e LLM_API_KEY="ollama" \
    -e LLM_BASE_URL="http://host.docker.internal:11434/v1" \
    --add-host=host.docker.internal:host-gateway \
    ghcr.io/all-hands-ai/openhands:main
EOF
chmod +x ~/workspace/scripts/start-openhands.sh

# ì ‘ì†: http://localhost:3001
```

---

## Phase 4: Vision AI ì‹œìŠ¤í…œ êµ¬ì¶•

### 4.1 Vision AI í™˜ê²½ êµ¬ì„±

```bash
# Vision AI ì „ìš© ê°€ìƒí™˜ê²½
cd ~/workspace/vision
python3 -m venv vision-env
source vision-env/bin/activate

# PyTorch ì„¤ì¹˜ (ARM64 + CUDA)
pip install torch torchvision torchaudio

# ì„¤ì¹˜ í™•ì¸
python3 << 'EOF'
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
EOF
```

### 4.2 Computer Vision ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
source ~/workspace/vision/vision-env/bin/activate

# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install \
    opencv-python \
    opencv-contrib-python \
    numpy \
    scipy \
    scikit-image \
    scikit-learn \
    pillow \
    albumentations \
    imageio \
    imageio-ffmpeg

# ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
pip install \
    transformers \
    accelerate \
    datasets \
    timm \
    einops \
    safetensors

# Object Detection
pip install ultralytics  # YOLO

# ì‹œê°í™”
pip install \
    matplotlib \
    seaborn \
    plotly
```

### 4.3 Vision Language Model ì„¤ì¹˜

```bash
# Ollamaë¡œ Vision LLM ì„¤ì¹˜

# Qwen2.5-VL (ì£¼ë ¥ Vision LLM)
# - ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì´í•´, OCR, ê°ì²´ ì¸ì‹
# - 29ê°œ ì–¸ì–´ ì§€ì›
ollama pull qwen2.5-vl:7b
ollama pull qwen2.5-vl:72b  # ëŒ€í˜• ëª¨ë¸ (ê³ í’ˆì§ˆ ë¶„ì„ìš©)

# LLaVA (ëŒ€ì•ˆ)
ollama pull llava:13b

# í…ŒìŠ¤íŠ¸
ollama run qwen2.5-vl:7b "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
```

### 4.4 Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
source ~/workspace/vision/vision-env/bin/activate

# HF CLI ì„¤ì¹˜
pip install huggingface_hub[cli]

# ì˜¤í”„ë¼ì¸ ì‚¬ìš©ì„ ìœ„í•œ ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ

# Depth Estimation
huggingface-cli download depth-anything/Depth-Anything-V2-Large \
    --local-dir $HF_HOME/depth-anything-v2-large

# Segmentation (SAM2)
huggingface-cli download facebook/sam2-hiera-large \
    --local-dir $HF_HOME/sam2-hiera-large

# CLIP (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì„ë² ë”©)
huggingface-cli download openai/clip-vit-large-patch14 \
    --local-dir $HF_HOME/clip-vit-large
```

### 4.5 YOLO ì„¤ì •

```bash
source ~/workspace/vision/vision-env/bin/activate

# YOLO ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° í…ŒìŠ¤íŠ¸
python3 << 'EOF'
from ultralytics import YOLO

# YOLOv8 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
model = YOLO('yolov8x.pt')  # Extra large model
print("YOLOv8 loaded successfully!")

# ëª¨ë¸ ì •ë³´
print(f"Model: {model.model}")
print(f"Task: {model.task}")
EOF
```

### 4.6 Segment Anything Model 2 (SAM2)

```bash
source ~/workspace/vision/vision-env/bin/activate

cd ~/workspace/vision
git clone https://github.com/facebookresearch/segment-anything-2.git
cd segment-anything-2

pip install -e .

# ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ
mkdir -p checkpoints
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
```

### 4.7 Jupyter Lab ì„¤ì •

```bash
source ~/workspace/vision/vision-env/bin/activate

pip install jupyterlab ipywidgets jupyter-ai

# Jupyter ì„¤ì •
jupyter lab --generate-config
jupyter lab password

# ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
sudo tee /etc/systemd/system/jupyter.service << EOF
[Unit]
Description=Jupyter Lab Server
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$HOME/workspace
Environment="PATH=$HOME/workspace/vision/vision-env/bin:/usr/local/bin:/usr/bin"
ExecStart=$HOME/workspace/vision/vision-env/bin/jupyter lab --ip=0.0.0.0 --port=8888 --no-browser
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable jupyter
sudo systemctl start jupyter

# ì ‘ì†: http://localhost:8888
```

---

## Phase 5: í†µí•© ê´€ë¦¬ ë° ìë™í™”

### 5.1 ì„œë¹„ìŠ¤ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```bash
# ì „ì²´ ì„œë¹„ìŠ¤ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
cat > ~/workspace/scripts/start-all.sh << 'EOF'
#!/bin/bash
echo "=========================================="
echo "    GX10 AI Services Starting...          "
echo "=========================================="

# 1. Ollama
echo "[1/4] Starting Ollama..."
sudo systemctl start ollama
sleep 3

# 2. Open WebUI
echo "[2/4] Starting Open WebUI..."
docker start open-webui 2>/dev/null || \
    docker run -d --name open-webui --restart always --gpus all \
    -p 8080:8080 -v open-webui-data:/app/backend/data \
    -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
    --add-host=host.docker.internal:host-gateway \
    ghcr.io/open-webui/open-webui:main

# 3. Jupyter Lab
echo "[3/4] Starting Jupyter Lab..."
sudo systemctl start jupyter

# 4. ìƒíƒœ í™•ì¸
echo "[4/4] Checking services..."
sleep 2

echo ""
echo "=========================================="
echo "    Services Ready                        "
echo "=========================================="
echo "  Open WebUI  : http://localhost:8080"
echo "  Jupyter Lab : http://localhost:8888"
echo "  Ollama API  : http://localhost:11434"
echo "=========================================="
EOF
chmod +x ~/workspace/scripts/start-all.sh
```

```bash
# ìƒíƒœ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
cat > ~/workspace/scripts/status.sh << 'EOF'
#!/bin/bash
echo "=========================================="
echo "    GX10 System Status                    "
echo "=========================================="

echo ""
echo "ğŸ“Š GPU Status:"
nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu --format=csv,noheader

echo ""
echo "ğŸ’¾ Memory:"
free -h | grep -E "Mem|Swap"

echo ""
echo "ğŸ³ Docker Containers:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" 2>/dev/null || echo "  Docker not running"

echo ""
echo "âš™ï¸  Services:"
systemctl is-active --quiet ollama && echo "  âœ… Ollama: Running" || echo "  âŒ Ollama: Stopped"
systemctl is-active --quiet jupyter && echo "  âœ… Jupyter: Running" || echo "  âŒ Jupyter: Stopped"
docker ps -q -f name=open-webui > /dev/null 2>&1 && echo "  âœ… Open WebUI: Running" || echo "  âŒ Open WebUI: Stopped"

echo ""
echo "ğŸ¤– Ollama Models:"
ollama list 2>/dev/null || echo "  Cannot connect to Ollama"

echo ""
echo "ğŸ’¿ Storage:"
df -h / | tail -1 | awk '{print "  Used: "$3" / "$2" ("$5")"}'
EOF
chmod +x ~/workspace/scripts/status.sh
```

```bash
# í™˜ê²½ í™œì„±í™” ìŠ¤í¬ë¦½íŠ¸
cat > ~/workspace/scripts/activate-coding.sh << 'EOF'
#!/bin/bash
echo "ğŸš€ Coding Agent Environment"
echo ""

# Ollama í™•ì¸
if curl -s http://localhost:11434/api/version > /dev/null 2>&1; then
    echo "âœ… Ollama: Connected"
    echo "   Models: $(ollama list 2>/dev/null | tail -n +2 | wc -l)"
else
    echo "âš ï¸  Starting Ollama..."
    sudo systemctl start ollama
fi

echo ""
echo "Available commands:"
echo "  aider              - CLI pair programming"
echo "  code .             - VS Code with Cline/Continue"
echo "  ollama run <model> - Direct model interaction"
EOF
chmod +x ~/workspace/scripts/activate-coding.sh

cat > ~/workspace/scripts/activate-vision.sh << 'EOF'
#!/bin/bash
echo "ğŸ‘ï¸  Vision AI Environment"
source ~/workspace/vision/vision-env/bin/activate
echo ""
echo "âœ… Python environment activated"
echo ""
echo "Available tools:"
echo "  jupyter lab - Notebook interface"
echo "  python      - PyTorch/Vision environment"
echo "  yolo        - Object detection CLI"
EOF
chmod +x ~/workspace/scripts/activate-vision.sh
```

### 5.2 ë¶€íŒ…ì‹œ ìë™ ì‹œì‘

```bash
# crontab ë“±ë¡
(crontab -l 2>/dev/null; echo "@reboot sleep 60 && $HOME/workspace/scripts/start-all.sh >> $HOME/workspace/logs/startup.log 2>&1") | crontab -

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/workspace/logs
```

### 5.3 Bash ë³„ì¹­ ì¶”ê°€

```bash
cat >> ~/.bashrc << 'EOF'

# GX10 AI System Aliases
alias ai-start="~/workspace/scripts/start-all.sh"
alias ai-status="~/workspace/scripts/status.sh"
alias ai-coding="source ~/workspace/scripts/activate-coding.sh"
alias ai-vision="source ~/workspace/scripts/activate-vision.sh"

# Quick model access
alias chat="ollama run qwen2.5-coder:32b"
alias chat-fast="ollama run qwen2.5-coder:7b"
alias vision="ollama run qwen2.5-vl:7b"
EOF

source ~/.bashrc
```

---

## Phase 6: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 6.1 Coding Agent í…ŒìŠ¤íŠ¸

```bash
# Aider í…ŒìŠ¤íŠ¸
ai-coding
cd ~/workspace/projects
mkdir hello-world && cd hello-world
git init

aider --model ollama/qwen2.5-coder:32b

# Aider í”„ë¡¬í”„íŠ¸ì—ì„œ:
# > Create a REST API with FastAPI that has CRUD operations for a todo list
# > Add unit tests using pytest
# > Generate documentation
```

### 6.2 Vision AI í…ŒìŠ¤íŠ¸

```bash
ai-vision

python3 << 'EOF'
import torch
from ultralytics import YOLO
from transformers import pipeline

print("=" * 50)
print("Vision AI Environment Test")
print("=" * 50)

# PyTorch
print(f"\nâœ… PyTorch: {torch.__version__}")
print(f"   CUDA: {torch.cuda.is_available()}")

# YOLO
model = YOLO('yolov8n.pt')
print(f"\nâœ… YOLO: Loaded successfully")

# Transformers
print(f"\nâœ… Transformers: Ready")

print("\n" + "=" * 50)
print("All tests passed!")
print("=" * 50)
EOF
```

### 6.3 í†µí•© í…ŒìŠ¤íŠ¸

```bash
# ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
ai-status

# ê° ì„œë¹„ìŠ¤ ì ‘ì† í…ŒìŠ¤íŠ¸
curl -s http://localhost:11434/api/version | jq .
curl -s http://localhost:8080 > /dev/null && echo "Open WebUI: OK"
curl -s http://localhost:8888 > /dev/null && echo "Jupyter: OK"
```

---

## ë¹ ë¥¸ ì°¸ì¡°

### ì„œë¹„ìŠ¤ ì ‘ì† URL

| Service | URL | Description |
|---------|-----|-------------|
| Open WebUI | http://localhost:8080 | ì›¹ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ |
| Jupyter Lab | http://localhost:8888 | ë…¸íŠ¸ë¶ í™˜ê²½ |
| Ollama API | http://localhost:11434 | LLM API ì—”ë“œí¬ì¸íŠ¸ |
| OpenHands | http://localhost:3001 | ììœ¨ ê°œë°œ ì—ì´ì „íŠ¸ |

### ê¶Œì¥ ëª¨ë¸ êµ¬ì„±

| ìš©ë„ | ëª¨ë¸ | í¬ê¸° | ë¹„ê³  |
|------|------|------|------|
| ì½”ë”© (ë©”ì¸) | qwen2.5-coder:32b | ~20GB | ìµœê³  ì„±ëŠ¥ |
| ì½”ë”© (ë¹ ë¦„) | qwen2.5-coder:7b | ~4GB | ìë™ì™„ì„± |
| ì½”ë”© (ëŒ€ì•ˆ) | deepseek-coder-v2:16b | ~10GB | ìˆ˜í•™/ë…¼ë¦¬ ê°•ì  |
| ì½”ë”© (ëŒ€ì•ˆ) | codestral:22b | ~13GB | Mistral |
| Vision | qwen2.5-vl:7b | ~5GB | ì´ë¯¸ì§€/OCR |
| Vision (ê³ í’ˆì§ˆ) | qwen2.5-vl:72b | ~45GB | ë³µì¡í•œ ë¶„ì„ |
| ì„ë² ë”© | nomic-embed-text | ~275MB | ì½”ë“œ ê²€ìƒ‰ |

### ëª…ë ¹ì–´ ìš”ì•½

```bash
# ì„œë¹„ìŠ¤ ê´€ë¦¬
ai-start          # ì „ì²´ ì„œë¹„ìŠ¤ ì‹œì‘
ai-status         # ìƒíƒœ í™•ì¸

# í™˜ê²½ í™œì„±í™”
ai-coding         # ì½”ë”© í™˜ê²½
ai-vision         # Vision AI í™˜ê²½

# ë¹ ë¥¸ ì±„íŒ…
chat              # 32B ì½”ë”© ëª¨ë¸
chat-fast         # 7B ë¹ ë¥¸ ì‘ë‹µ
vision            # Vision LLM

# Ollama ê´€ë¦¬
ollama list       # ì„¤ì¹˜ëœ ëª¨ë¸
ollama ps         # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸
ollama stop <model>  # ëª¨ë¸ ì¤‘ì§€
```

---

## ë¬¸ì œ í•´ê²°

### Ollama ì—°ê²° ë¬¸ì œ

```bash
# ì„œë¹„ìŠ¤ ì¬ì‹œì‘
sudo systemctl restart ollama

# ë¡œê·¸ í™•ì¸
journalctl -u ollama -f

# í¬íŠ¸ í™•ì¸
ss -tlnp | grep 11434
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸ ë° ì •ë¦¬
ollama ps
ollama stop <model-name>

# GPU ë©”ëª¨ë¦¬ ìƒíƒœ
nvidia-smi

# ìºì‹œ ì •ë¦¬
sync && echo 3 | sudo tee /proc/sys/vm/drop_caches
```

### Docker ê¶Œí•œ ë¬¸ì œ

```bash
sudo usermod -aG docker $USER
newgrp docker
# ë˜ëŠ” ì¬ë¡œê·¸ì¸
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

```bash
# ì¬ì‹œë„
ollama pull <model-name>

# ë„¤íŠ¸ì›Œí¬ í™•ì¸
curl -I https://ollama.com

# ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í›„ import
# ollama create <name> -f Modelfile
```
---

## ğŸ“ ë¬¸ì„œ ì •ë³´

**ì‘ì„±ì**:

- AI: Claude Sonnet 4.5
- í™˜ê²½: MoAI-ADK v11.0.0
- ì‘ì„±ì¼: 2026-02-01

**ë¦¬ë·°ì–´**:

- drake

